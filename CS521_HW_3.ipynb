{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scdivad/CS521/blob/main/CS521_HW_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmALG91XRvWx",
        "outputId": "04524788-efa0-4b97-bbd5-17130c5cb61a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whsg1XX_OZs6"
      },
      "source": [
        "# Q1 setep\n",
        "Do not run for Q2 due to dependency issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1domTvnONqD",
        "outputId": "9a580f7a-ca23-4fc0-c15a-5d1e865412a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (relu1): ReLU()\n",
              "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
              "  (relu2): ReLU()\n",
              "  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "use_cuda = False\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "batch_size = 64\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "## Dataloaders\n",
        "train_dataset = datasets.MNIST('mnist_data/', train=True, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "test_dataset = datasets.MNIST('mnist_data/', train=False, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc = nn.Linear(28*28, 50)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(50,50)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(50,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view((-1, 28*28))\n",
        "        x = self.fc(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "\n",
        "model = model.to(device)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmxQfb0rfXlc"
      },
      "outputs": [],
      "source": [
        "def test_model(model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print(f'Accuracy on images: {100 * correct / total}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2ZmDqiaAxLS"
      },
      "outputs": [],
      "source": [
        "# HW1\n",
        "\n",
        "# The last argument 'targeted' can be used to toggle between a targeted and untargeted attack.\n",
        "# param labels should be y if target is false and target labels if target is true\n",
        "def fgsm(model, x, labels, eps_step, targeted):\n",
        "    model.eval()\n",
        "    x.requires_grad_()\n",
        "    outputs = model(x)\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    adv_x = x + (-1 if targeted else +1) * eps_step * x.grad.sign()\n",
        "    return torch.clamp(adv_x, 0, 1)\n",
        "\n",
        "def pgd_untargeted(model, x, y, k, eps, eps_step):\n",
        "    model.eval()\n",
        "    lb = x - eps\n",
        "    ub = x + eps\n",
        "    for i in range(k):\n",
        "        x = fgsm(model, x, y, eps_step, targeted=False).detach()\n",
        "        x = torch.clamp(x, lb, ub)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zBaVo77Ao6t"
      },
      "outputs": [],
      "source": [
        "# HW1\n",
        "def test_model_on_attacks(model, attack='pgd', attack_epochs=20, eps=0.1, c=1, num_examples_show=0):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    original_images_list = []\n",
        "    attacked_images_list = []\n",
        "    labels_list = []\n",
        "    predictions_list = []\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        correct_indices = torch.max(model(images), 1)[1] == labels\n",
        "        if attack == 'pgd':\n",
        "            images_attacked = pgd_untargeted(model, images, labels, attack_epochs, eps, 0.01)\n",
        "        else:\n",
        "            images_attacked = images\n",
        "\n",
        "        outputs = model(images_attacked)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # only attach images that were correctly classified to begin with\n",
        "        if num_examples_show:\n",
        "            original_images_list.append(images[correct_indices].cpu().detach().numpy())\n",
        "            attacked_images_list.append(images_attacked[correct_indices].cpu().detach().numpy())\n",
        "            labels_list.append(labels[correct_indices].cpu().detach().numpy())\n",
        "            predictions_list.append(predicted[correct_indices].cpu().detach().numpy())\n",
        "\n",
        "    if num_examples_show:\n",
        "        original_images_list = np.concatenate(original_images_list)\n",
        "        attacked_images_list = np.concatenate(attacked_images_list)\n",
        "        labels_list = np.concatenate(labels_list)\n",
        "        predictions_list = np.concatenate(predictions_list)\n",
        "\n",
        "        num_samples = min(num_examples_show, original_images_list.shape[0])\n",
        "        indices = np.random.choice(original_images_list.shape[0], num_samples, replace=False)\n",
        "\n",
        "        for i in indices:\n",
        "            plt.figure(figsize=(5, 2.5))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(original_images_list[i].reshape(28, 28), cmap='gray')\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(attacked_images_list[i].reshape(28, 28), cmap='gray')\n",
        "            plt.title(f'{\"Successful\" if predictions_list[i] != labels_list[i] else \"Unsuccessful\"} Attack: {predictions_list[i]}')\n",
        "            plt.show()\n",
        "    print(f'Robust accuracy on {attack} {eps} images: {100 * correct / total}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPMdfEhtR3zm"
      },
      "source": [
        "# Q1 main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqG0L35GsRLD"
      },
      "outputs": [],
      "source": [
        "def IBP_bounds(net, x0, eps):\n",
        "    lower = x0 - eps\n",
        "    upper = x0 + eps\n",
        "    num_layers = len(list(net.children()))\n",
        "    for i, layer in enumerate(net.children()):\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            W = layer.weight\n",
        "            b = layer.bias\n",
        "            pos_W = torch.clamp(W, min=0)\n",
        "            neg_W = torch.clamp(W, max=0)\n",
        "            lower2 = upper @ neg_W.T + lower @ pos_W.T + b\n",
        "            upper2 = upper @ pos_W.T + lower @ neg_W.T + b\n",
        "            lower, upper = lower2, upper2\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            lower = torch.clamp(lower, min=0)\n",
        "            upper = torch.clamp(upper, min=0)\n",
        "\n",
        "    return (lower, upper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWi5tMGreqmK"
      },
      "outputs": [],
      "source": [
        "def train_model(model, num_epochs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    kappa = 1\n",
        "    eps = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            batch_size = images.shape[0]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            ibp_lb, ibp_ub = IBP_bounds(model, images.view(batch_size, -1), eps)\n",
        "            z = ibp_ub # this is a pointer, careful\n",
        "            z[torch.arange(batch_size), labels] = ibp_lb[torch.arange(batch_size), labels]\n",
        "            z = torch.softmax(z, dim=-1)\n",
        "            loss = kappa * criterion(outputs, labels) + (1 - kappa) * criterion(z, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.3f}')\n",
        "\n",
        "        kappa -= 0.5 / (num_epochs-1)\n",
        "        eps += 0.1 / (num_epochs-1)\n",
        "        test_model(model)\n",
        "        test_model_on_attacks(model, attack='pgd', attack_epochs=20, eps=0.1, num_examples_show=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhfeZe1ZzOv2",
        "outputId": "1186434d-1e93-464d-97e0-c3a98c1bc159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 0.416\n",
            "Accuracy on images: 93.62\n",
            "Robust accuracy on pgd 0.1 images: 10.33\n",
            "Epoch 2/100, Loss: 0.201\n",
            "Accuracy on images: 95.29\n",
            "Robust accuracy on pgd 0.1 images: 8.07\n",
            "Epoch 3/100, Loss: 0.161\n",
            "Accuracy on images: 95.92\n",
            "Robust accuracy on pgd 0.1 images: 8.66\n",
            "Epoch 4/100, Loss: 0.144\n",
            "Accuracy on images: 96.56\n",
            "Robust accuracy on pgd 0.1 images: 9.12\n",
            "Epoch 5/100, Loss: 0.137\n",
            "Accuracy on images: 96.33\n",
            "Robust accuracy on pgd 0.1 images: 12.43\n",
            "Epoch 6/100, Loss: 0.135\n",
            "Accuracy on images: 96.89\n",
            "Robust accuracy on pgd 0.1 images: 12.58\n",
            "Epoch 7/100, Loss: 0.137\n",
            "Accuracy on images: 97.12\n",
            "Robust accuracy on pgd 0.1 images: 15.81\n",
            "Epoch 8/100, Loss: 0.140\n",
            "Accuracy on images: 97.05\n",
            "Robust accuracy on pgd 0.1 images: 15.32\n",
            "Epoch 9/100, Loss: 0.147\n",
            "Accuracy on images: 97.07\n",
            "Robust accuracy on pgd 0.1 images: 15.34\n",
            "Epoch 10/100, Loss: 0.153\n",
            "Accuracy on images: 97.14\n",
            "Robust accuracy on pgd 0.1 images: 20.95\n",
            "Epoch 11/100, Loss: 0.162\n",
            "Accuracy on images: 97.22\n",
            "Robust accuracy on pgd 0.1 images: 19.66\n",
            "Epoch 12/100, Loss: 0.168\n",
            "Accuracy on images: 97.05\n",
            "Robust accuracy on pgd 0.1 images: 22.29\n",
            "Epoch 13/100, Loss: 0.178\n",
            "Accuracy on images: 97.23\n",
            "Robust accuracy on pgd 0.1 images: 22.1\n",
            "Epoch 14/100, Loss: 0.187\n",
            "Accuracy on images: 97.28\n",
            "Robust accuracy on pgd 0.1 images: 21.08\n",
            "Epoch 15/100, Loss: 0.196\n",
            "Accuracy on images: 97.25\n",
            "Robust accuracy on pgd 0.1 images: 21.04\n",
            "Epoch 16/100, Loss: 0.206\n",
            "Accuracy on images: 97.17\n",
            "Robust accuracy on pgd 0.1 images: 21.1\n",
            "Epoch 17/100, Loss: 0.216\n",
            "Accuracy on images: 97.43\n",
            "Robust accuracy on pgd 0.1 images: 21.16\n",
            "Epoch 18/100, Loss: 0.225\n",
            "Accuracy on images: 97.32\n",
            "Robust accuracy on pgd 0.1 images: 22.12\n",
            "Epoch 19/100, Loss: 0.236\n",
            "Accuracy on images: 97.25\n",
            "Robust accuracy on pgd 0.1 images: 21.59\n",
            "Epoch 20/100, Loss: 0.246\n",
            "Accuracy on images: 97.45\n",
            "Robust accuracy on pgd 0.1 images: 21.99\n",
            "Epoch 21/100, Loss: 0.257\n",
            "Accuracy on images: 97.3\n",
            "Robust accuracy on pgd 0.1 images: 22.0\n",
            "Epoch 22/100, Loss: 0.266\n",
            "Accuracy on images: 97.35\n",
            "Robust accuracy on pgd 0.1 images: 22.59\n",
            "Epoch 23/100, Loss: 0.279\n",
            "Accuracy on images: 97.32\n",
            "Robust accuracy on pgd 0.1 images: 21.96\n",
            "Epoch 24/100, Loss: 0.286\n",
            "Accuracy on images: 97.12\n",
            "Robust accuracy on pgd 0.1 images: 21.71\n",
            "Epoch 25/100, Loss: 0.298\n",
            "Accuracy on images: 97.24\n",
            "Robust accuracy on pgd 0.1 images: 23.28\n",
            "Epoch 26/100, Loss: 0.310\n",
            "Accuracy on images: 96.99\n",
            "Robust accuracy on pgd 0.1 images: 22.97\n",
            "Epoch 27/100, Loss: 0.320\n",
            "Accuracy on images: 97.15\n",
            "Robust accuracy on pgd 0.1 images: 23.43\n",
            "Epoch 28/100, Loss: 0.333\n",
            "Accuracy on images: 97.25\n",
            "Robust accuracy on pgd 0.1 images: 23.49\n",
            "Epoch 29/100, Loss: 0.343\n",
            "Accuracy on images: 97.18\n",
            "Robust accuracy on pgd 0.1 images: 23.82\n",
            "Epoch 30/100, Loss: 0.354\n",
            "Accuracy on images: 97.08\n",
            "Robust accuracy on pgd 0.1 images: 24.31\n",
            "Epoch 31/100, Loss: 0.365\n",
            "Accuracy on images: 97.2\n",
            "Robust accuracy on pgd 0.1 images: 24.35\n",
            "Epoch 32/100, Loss: 0.377\n",
            "Accuracy on images: 97.26\n",
            "Robust accuracy on pgd 0.1 images: 24.55\n",
            "Epoch 33/100, Loss: 0.388\n",
            "Accuracy on images: 96.88\n",
            "Robust accuracy on pgd 0.1 images: 23.64\n",
            "Epoch 34/100, Loss: 0.400\n",
            "Accuracy on images: 97.4\n",
            "Robust accuracy on pgd 0.1 images: 24.1\n",
            "Epoch 35/100, Loss: 0.409\n",
            "Accuracy on images: 97.26\n",
            "Robust accuracy on pgd 0.1 images: 24.27\n",
            "Epoch 36/100, Loss: 0.421\n",
            "Accuracy on images: 96.84\n",
            "Robust accuracy on pgd 0.1 images: 25.22\n",
            "Epoch 37/100, Loss: 0.433\n",
            "Accuracy on images: 97.23\n",
            "Robust accuracy on pgd 0.1 images: 24.69\n",
            "Epoch 38/100, Loss: 0.444\n",
            "Accuracy on images: 97.01\n",
            "Robust accuracy on pgd 0.1 images: 23.9\n",
            "Epoch 39/100, Loss: 0.455\n",
            "Accuracy on images: 97.03\n",
            "Robust accuracy on pgd 0.1 images: 25.35\n",
            "Epoch 40/100, Loss: 0.465\n",
            "Accuracy on images: 97.37\n",
            "Robust accuracy on pgd 0.1 images: 26.05\n",
            "Epoch 41/100, Loss: 0.477\n",
            "Accuracy on images: 97.14\n",
            "Robust accuracy on pgd 0.1 images: 25.59\n",
            "Epoch 42/100, Loss: 0.489\n",
            "Accuracy on images: 97.11\n",
            "Robust accuracy on pgd 0.1 images: 26.76\n",
            "Epoch 43/100, Loss: 0.499\n",
            "Accuracy on images: 97.21\n",
            "Robust accuracy on pgd 0.1 images: 26.11\n",
            "Epoch 44/100, Loss: 0.512\n",
            "Accuracy on images: 97.13\n",
            "Robust accuracy on pgd 0.1 images: 25.38\n",
            "Epoch 45/100, Loss: 0.524\n",
            "Accuracy on images: 97.05\n",
            "Robust accuracy on pgd 0.1 images: 26.55\n",
            "Epoch 46/100, Loss: 0.533\n",
            "Accuracy on images: 96.99\n",
            "Robust accuracy on pgd 0.1 images: 26.34\n",
            "Epoch 47/100, Loss: 0.545\n",
            "Accuracy on images: 97.08\n",
            "Robust accuracy on pgd 0.1 images: 27.2\n",
            "Epoch 48/100, Loss: 0.557\n",
            "Accuracy on images: 97.06\n",
            "Robust accuracy on pgd 0.1 images: 26.04\n",
            "Epoch 49/100, Loss: 0.567\n",
            "Accuracy on images: 97.06\n",
            "Robust accuracy on pgd 0.1 images: 26.61\n",
            "Epoch 50/100, Loss: 0.581\n",
            "Accuracy on images: 97.13\n",
            "Robust accuracy on pgd 0.1 images: 27.48\n",
            "Epoch 51/100, Loss: 0.591\n",
            "Accuracy on images: 97.08\n",
            "Robust accuracy on pgd 0.1 images: 26.74\n",
            "Epoch 52/100, Loss: 0.603\n",
            "Accuracy on images: 97.15\n",
            "Robust accuracy on pgd 0.1 images: 26.64\n",
            "Epoch 53/100, Loss: 0.615\n",
            "Accuracy on images: 97.15\n",
            "Robust accuracy on pgd 0.1 images: 28.11\n",
            "Epoch 54/100, Loss: 0.625\n",
            "Accuracy on images: 97.08\n",
            "Robust accuracy on pgd 0.1 images: 26.6\n",
            "Epoch 55/100, Loss: 0.637\n",
            "Accuracy on images: 97.18\n",
            "Robust accuracy on pgd 0.1 images: 27.85\n",
            "Epoch 56/100, Loss: 0.649\n",
            "Accuracy on images: 97.24\n",
            "Robust accuracy on pgd 0.1 images: 26.83\n",
            "Epoch 57/100, Loss: 0.659\n",
            "Accuracy on images: 97.18\n",
            "Robust accuracy on pgd 0.1 images: 28.24\n",
            "Epoch 58/100, Loss: 0.670\n",
            "Accuracy on images: 97.1\n",
            "Robust accuracy on pgd 0.1 images: 27.51\n",
            "Epoch 59/100, Loss: 0.683\n",
            "Accuracy on images: 97.23\n",
            "Robust accuracy on pgd 0.1 images: 27.73\n",
            "Epoch 60/100, Loss: 0.694\n",
            "Accuracy on images: 96.96\n",
            "Robust accuracy on pgd 0.1 images: 26.9\n",
            "Epoch 61/100, Loss: 0.706\n",
            "Accuracy on images: 96.92\n",
            "Robust accuracy on pgd 0.1 images: 28.23\n",
            "Epoch 62/100, Loss: 0.718\n",
            "Accuracy on images: 96.78\n",
            "Robust accuracy on pgd 0.1 images: 28.44\n",
            "Epoch 63/100, Loss: 0.728\n",
            "Accuracy on images: 96.96\n",
            "Robust accuracy on pgd 0.1 images: 27.87\n",
            "Epoch 64/100, Loss: 0.739\n",
            "Accuracy on images: 96.91\n",
            "Robust accuracy on pgd 0.1 images: 28.02\n",
            "Epoch 65/100, Loss: 0.753\n",
            "Accuracy on images: 96.93\n",
            "Robust accuracy on pgd 0.1 images: 27.68\n",
            "Epoch 66/100, Loss: 0.763\n",
            "Accuracy on images: 97.13\n",
            "Robust accuracy on pgd 0.1 images: 28.85\n",
            "Epoch 67/100, Loss: 0.777\n",
            "Accuracy on images: 96.95\n",
            "Robust accuracy on pgd 0.1 images: 27.44\n",
            "Epoch 68/100, Loss: 0.787\n",
            "Accuracy on images: 96.72\n",
            "Robust accuracy on pgd 0.1 images: 28.32\n",
            "Epoch 69/100, Loss: 0.798\n",
            "Accuracy on images: 96.96\n",
            "Robust accuracy on pgd 0.1 images: 28.62\n",
            "Epoch 70/100, Loss: 0.809\n",
            "Accuracy on images: 96.7\n",
            "Robust accuracy on pgd 0.1 images: 27.97\n",
            "Epoch 71/100, Loss: 0.823\n",
            "Accuracy on images: 96.65\n",
            "Robust accuracy on pgd 0.1 images: 28.09\n",
            "Epoch 72/100, Loss: 0.833\n",
            "Accuracy on images: 96.75\n",
            "Robust accuracy on pgd 0.1 images: 29.5\n",
            "Epoch 73/100, Loss: 0.844\n",
            "Accuracy on images: 96.91\n",
            "Robust accuracy on pgd 0.1 images: 28.78\n",
            "Epoch 74/100, Loss: 0.857\n",
            "Accuracy on images: 97.08\n",
            "Robust accuracy on pgd 0.1 images: 29.39\n",
            "Epoch 75/100, Loss: 0.870\n",
            "Accuracy on images: 96.87\n",
            "Robust accuracy on pgd 0.1 images: 28.65\n",
            "Epoch 76/100, Loss: 0.878\n",
            "Accuracy on images: 96.71\n",
            "Robust accuracy on pgd 0.1 images: 28.76\n",
            "Epoch 77/100, Loss: 0.892\n",
            "Accuracy on images: 96.79\n",
            "Robust accuracy on pgd 0.1 images: 29.53\n",
            "Epoch 78/100, Loss: 0.902\n",
            "Accuracy on images: 96.74\n",
            "Robust accuracy on pgd 0.1 images: 29.57\n",
            "Epoch 79/100, Loss: 0.915\n",
            "Accuracy on images: 96.8\n",
            "Robust accuracy on pgd 0.1 images: 29.47\n",
            "Epoch 80/100, Loss: 0.924\n",
            "Accuracy on images: 96.72\n",
            "Robust accuracy on pgd 0.1 images: 29.02\n",
            "Epoch 81/100, Loss: 0.939\n",
            "Accuracy on images: 96.94\n",
            "Robust accuracy on pgd 0.1 images: 29.21\n",
            "Epoch 82/100, Loss: 0.949\n",
            "Accuracy on images: 97.04\n",
            "Robust accuracy on pgd 0.1 images: 29.62\n",
            "Epoch 83/100, Loss: 0.961\n",
            "Accuracy on images: 96.77\n",
            "Robust accuracy on pgd 0.1 images: 30.07\n",
            "Epoch 84/100, Loss: 0.974\n",
            "Accuracy on images: 96.81\n",
            "Robust accuracy on pgd 0.1 images: 29.11\n",
            "Epoch 85/100, Loss: 0.983\n",
            "Accuracy on images: 96.71\n",
            "Robust accuracy on pgd 0.1 images: 30.53\n",
            "Epoch 86/100, Loss: 0.996\n",
            "Accuracy on images: 96.74\n",
            "Robust accuracy on pgd 0.1 images: 29.8\n",
            "Epoch 87/100, Loss: 1.008\n",
            "Accuracy on images: 96.87\n",
            "Robust accuracy on pgd 0.1 images: 29.32\n",
            "Epoch 88/100, Loss: 1.020\n",
            "Accuracy on images: 96.85\n",
            "Robust accuracy on pgd 0.1 images: 30.31\n",
            "Epoch 89/100, Loss: 1.031\n",
            "Accuracy on images: 96.76\n",
            "Robust accuracy on pgd 0.1 images: 29.84\n",
            "Epoch 90/100, Loss: 1.044\n",
            "Accuracy on images: 96.93\n",
            "Robust accuracy on pgd 0.1 images: 29.24\n",
            "Epoch 91/100, Loss: 1.054\n",
            "Accuracy on images: 96.68\n",
            "Robust accuracy on pgd 0.1 images: 30.82\n",
            "Epoch 92/100, Loss: 1.067\n",
            "Accuracy on images: 96.68\n",
            "Robust accuracy on pgd 0.1 images: 29.77\n",
            "Epoch 93/100, Loss: 1.078\n",
            "Accuracy on images: 96.76\n",
            "Robust accuracy on pgd 0.1 images: 29.69\n",
            "Epoch 94/100, Loss: 1.089\n",
            "Accuracy on images: 96.66\n",
            "Robust accuracy on pgd 0.1 images: 30.84\n",
            "Epoch 95/100, Loss: 1.101\n",
            "Accuracy on images: 96.91\n",
            "Robust accuracy on pgd 0.1 images: 30.23\n",
            "Epoch 96/100, Loss: 1.113\n",
            "Accuracy on images: 96.88\n",
            "Robust accuracy on pgd 0.1 images: 30.74\n",
            "Epoch 97/100, Loss: 1.124\n",
            "Accuracy on images: 96.61\n",
            "Robust accuracy on pgd 0.1 images: 30.66\n",
            "Epoch 98/100, Loss: 1.135\n",
            "Accuracy on images: 96.89\n",
            "Robust accuracy on pgd 0.1 images: 30.43\n",
            "Epoch 99/100, Loss: 1.149\n",
            "Accuracy on images: 96.56\n",
            "Robust accuracy on pgd 0.1 images: 30.15\n",
            "Epoch 100/100, Loss: 1.159\n",
            "Accuracy on images: 96.7\n",
            "Robust accuracy on pgd 0.1 images: 29.99\n"
          ]
        }
      ],
      "source": [
        "model = Net()\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_model(model, num_epochs=100)\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/CS521/weights_ibp_ellision_HW3.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1FmHe9j8BXM",
        "outputId": "3e79e7ec-74a2-402b-84c2-662e2cb433e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-29015c6eb8b7>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f'/content/drive/MyDrive/CS521/weights_ibp_ellision_HW3.pt'))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Net()\n",
        "model.load_state_dict(torch.load(f'/content/drive/MyDrive/CS521/weights_ibp_ellision_HW3.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXSubvGzpVCm",
        "outputId": "b6578df3-7cc8-45ff-9c22-16a303dcb6d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on images: 96.7\n"
          ]
        }
      ],
      "source": [
        "test_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jupNfIPIE9Oe",
        "outputId": "986a7d91-7a08-4150-e552-e52929773eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy on pgd 0.01 images: 94.33\n",
            "Robust accuracy on pgd 0.02 images: 90.0\n",
            "Robust accuracy on pgd 0.03 images: 83.01\n",
            "Robust accuracy on pgd 0.04 images: 72.78\n",
            "Robust accuracy on pgd 0.05 images: 61.99\n",
            "Robust accuracy on pgd 0.06 images: 51.94\n",
            "Robust accuracy on pgd 0.07 images: 44.01\n",
            "Robust accuracy on pgd 0.08 images: 38.21\n",
            "Robust accuracy on pgd 0.09 images: 33.59\n",
            "Robust accuracy on pgd 0.1 images: 29.99\n"
          ]
        }
      ],
      "source": [
        "for eps_test in [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.10]:\n",
        "    test_model_on_attacks(model, attack='pgd', eps=eps_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRpHzsQrFXty"
      },
      "outputs": [],
      "source": [
        "def test_model_verified_accuracy(model, eps=0.1):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        lower, upper = IBP_bounds(model, images.view(images.shape[0], -1), eps)\n",
        "        # count number of testing labels in batch where lb of the label is greater than the ub of any incorrect class\n",
        "        upper[torch.arange(images.shape[0]), labels] = -1e9\n",
        "        correct += (lower[torch.arange(images.shape[0]), labels] > upper.max(dim=-1)[0]).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    print(f'Verified accuracy: {100 * correct / total}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDLyMN7EFAGC",
        "outputId": "178e5e4b-0a02-4b0a-99ae-59bbea0c4d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified accuracy: 32.36\n",
            "Verified accuracy: 24.33\n",
            "Verified accuracy: 22.44\n",
            "Verified accuracy: 21.0\n",
            "Verified accuracy: 19.6\n",
            "Verified accuracy: 18.22\n",
            "Verified accuracy: 16.42\n",
            "Verified accuracy: 14.67\n",
            "Verified accuracy: 13.01\n",
            "Verified accuracy: 11.28\n"
          ]
        }
      ],
      "source": [
        "for eps_test in [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.10]:\n",
        "    test_model_verified_accuracy(model, eps=eps_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XQ6tDCOX7LI"
      },
      "source": [
        "# Q2 Setup part 1\n",
        "Libraries installation, imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSe5_I2V8lsK",
        "outputId": "6f37a3f4-4839-4e8d-c8a8-2df7b5276ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.3.0 torchtext==0.18.0 torchvision torchdata portalocker>=2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvzGhUx99GIb",
        "outputId": "93c59087-1b20-4fe7-f81b-ad6a9fa2d145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS521\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CS521"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mOeJE2bAV1LZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.datasets import IMDB\n",
        "from collections import Counter\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7bYUmSG0D9P3"
      },
      "outputs": [],
      "source": [
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GKddK_67XjN"
      },
      "source": [
        "# Q2 Setup part 2\n",
        "Model definition, dataset prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv1TXlxjoRgJ",
        "outputId": "1acaf7c4-0794-4489-aec9-09e1c911db74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12500, 12500)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "cnt1 = 0\n",
        "cnt2 = 0\n",
        "for label, line in IMDB(split='test'):\n",
        "    if label == 1: cnt1 += 1\n",
        "    else: cnt2 += 1\n",
        "cnt1, cnt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrtOW-FJ2xni",
        "outputId": "c1cbf234-1600-4ee1-dc1d-f63b82aa03c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "280618"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_iter = IMDB(split='train')\n",
        "imdbtoken_to_id = {}\n",
        "imdbtoken_to_id['<PAD>'] = 0\n",
        "for label, line in train_iter:\n",
        "    for token in line.split():\n",
        "        if token not in imdbtoken_to_id:\n",
        "            imdbtoken_to_id[token] = len(imdbtoken_to_id)\n",
        "len(imdbtoken_to_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgUKBMUnWXDq",
        "outputId": "b1d31b3d-e4eb-4842-b3ef-22cbca06e7dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85246"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "embeddings = {}\n",
        "d_embed = 300\n",
        "with open('glove.840B.300d.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        glove_line = line.strip().split()\n",
        "        glove_token = \"\".join(glove_line[:-d_embed])\n",
        "        glove_emb = torch.tensor([float(x) for x in glove_line[-d_embed:]], dtype=torch.float32)\n",
        "        if glove_token in imdbtoken_to_id:\n",
        "            embeddings[glove_token] = glove_emb\n",
        "len(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lfLu2vzybBYJ"
      },
      "outputs": [],
      "source": [
        "d_embed = 300\n",
        "d_vocab = len(imdbtoken_to_id)\n",
        "embedding_matrix = torch.zeros(d_vocab, d_embed)\n",
        "\n",
        "for word, id in imdbtoken_to_id.items():\n",
        "    if word in embeddings:\n",
        "        embedding_matrix[id] = embeddings[word]\n",
        "\n",
        "mean_embed = torch.mean(embedding_matrix, dim=0)\n",
        "embedding_matrix[0] = mean_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zLXU7r9dS-HE"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = embedding_matrix.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SY-lgUJZ-UsI"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data_iter, vocab, max_length):\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "\n",
        "        for label, line in data_iter:\n",
        "            tokens = line.split(' ')\n",
        "            indices = [vocab.get(token, vocab['<PAD>']) for token in tokens]\n",
        "            if len(indices) < max_length:\n",
        "                indices += [vocab['<PAD>']] * (max_length - len(indices))\n",
        "            else:\n",
        "                indices = indices[:max_length]\n",
        "            label_tensor = torch.tensor(label, dtype=torch.long) - 1  # Convert label to (0,1) instead of (1,2)\n",
        "            self.samples.append((torch.tensor(indices, dtype=torch.long), label_tensor))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "seq_length = 200\n",
        "train_dataset = IMDBDataset(IMDB(split='train'), imdbtoken_to_id, seq_length)\n",
        "test_dataset = IMDBDataset(IMDB(split='test'), imdbtoken_to_id, seq_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "qnXNK8uyQz1a"
      },
      "outputs": [],
      "source": [
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, pretrained_embeddings, seq_length):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "        self.linear_emb = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.relu_emb = nn.ReLU()\n",
        "\n",
        "        self.mean_linear = nn.Linear(self.seq_length, 1, bias=False)\n",
        "        with torch.no_grad():\n",
        "            self.mean_linear.weight.fill_(1.0 / self.seq_length)\n",
        "        self.mean_linear.weight.requires_grad = False\n",
        "\n",
        "        self.fc1 = nn.Linear(embedding_dim, 100)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.relu_emb(self.linear_emb(x))\n",
        "\n",
        "        # x0 = x.mean(dim=1)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.mean_linear(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        # assert torch.allclose(x, x0.unsqueeze(1))\n",
        "\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x.squeeze(1)\n",
        "\n",
        "    def forward_from_embedding(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.mean_linear(x)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x.squeeze(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2 Setup part 3\n",
        "Natural training, natural testing, empirical robustness testing, verified robustness testing"
      ],
      "metadata": {
        "id": "9E2xFaPC13HK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zve2w1hNtajy"
      },
      "outputs": [],
      "source": [
        "def test_natural_acc(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, batch_labels in data_loader:\n",
        "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "            outputs = model(batch_inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "    print(f'Natural accuracy: {correct / total}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Agmtz5V8b32P"
      },
      "outputs": [],
      "source": [
        "def test_model_verified_accuracy(model, eps=0.1):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        lower, upper = IBP_bounds_IMDB(model, model.embedding(images), eps)\n",
        "        # count number of testing labels in batch where lb of the label is greater than the ub of any incorrect class\n",
        "        upper[torch.arange(images.shape[0]), labels] = -1e9\n",
        "        correct += (lower[torch.arange(images.shape[0]), labels] > upper.max(dim=-1)[0]).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    print(f'Verified accuracy: {100 * correct / total}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "O2vcquj1-XGX"
      },
      "outputs": [],
      "source": [
        "def fgsm_IMDB(model, x_embed, labels, eps_step, targeted):\n",
        "    model.eval()\n",
        "    x_embed.requires_grad_()\n",
        "    outputs = model.forward_from_embedding(x_embed)\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    adv_x = x_embed + (-1 if targeted else +1) * eps_step * x_embed.grad.sign()\n",
        "    return torch.clamp(adv_x, 0, 1)\n",
        "\n",
        "def pgd_untargeted_IMDB(model, x, y, k, eps, eps_step):\n",
        "    model.eval()\n",
        "    x_embed = model.embedding(x)\n",
        "    lb = x_embed - eps\n",
        "    ub = x_embed + eps\n",
        "    for i in range(k):\n",
        "        x = fgsm(model, x_embed, y, eps_step, targeted=False).detach()\n",
        "        x = torch.clamp(x, lb, ub)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VVIJeo2bRULR"
      },
      "outputs": [],
      "source": [
        "def train_IMDB_nat(model, num_epochs):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_inputs, batch_labels in train_loader:\n",
        "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_inputs)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gdybII2pIoI1"
      },
      "outputs": [],
      "source": [
        "def test_IMDB_model_on_PGD(model, attack_epochs=20, eps=0.1):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    original_images_list = []\n",
        "    attacked_images_list = []\n",
        "    labels_list = []\n",
        "    predictions_list = []\n",
        "    for batch_inputs, batch_labels in test_loader:\n",
        "        batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "\n",
        "        correct_indices = torch.max(model(batch_inputs), 1)[1] == batch_labels\n",
        "        inputs_attacked = pgd_untargeted(model, batch_inputs, batch_labels, attack_epochs, eps, eps/10)\n",
        "\n",
        "        outputs = model.forward_from_embedding(inputs_attacked)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == batch_labels).sum().item()\n",
        "        total += batch_labels.size(0)\n",
        "\n",
        "    print(f'Robust accuracy on PGD {eps} images: {100 * correct / total}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVIB_86NQyed"
      },
      "source": [
        "# Q2 main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PHmnDETGKnDz"
      },
      "outputs": [],
      "source": [
        "def IBP_bounds_IMDB(net, embed_x0, eps):\n",
        "    lower = embed_x0 - eps\n",
        "    upper = embed_x0 + eps\n",
        "    num_layers = len(list(net.children()))\n",
        "    for i, layer in enumerate(net.children()):\n",
        "        if i == 0: continue # skip embed layer\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            W = layer.weight\n",
        "            b = layer.bias\n",
        "            if layer == net.mean_linear:\n",
        "                lower = lower.transpose(1, 2)\n",
        "                upper = upper.transpose(1, 2)\n",
        "                b = torch.zeros(W.shape[0], device=device)\n",
        "            pos_W = torch.clamp(W, min=0)\n",
        "            neg_W = torch.clamp(W, max=0)\n",
        "            lower2 = upper @ neg_W.T + lower @ pos_W.T + b\n",
        "            upper2 = upper @ pos_W.T + lower @ neg_W.T + b\n",
        "            lower, upper = lower2, upper2\n",
        "            if layer == net.mean_linear:\n",
        "                lower = lower.transpose(1, 2).squeeze(1)\n",
        "                upper = upper.transpose(1, 2).squeeze(1)\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            lower = torch.clamp(lower, min=0)\n",
        "            upper = torch.clamp(upper, min=0)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "    return (lower, upper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niDOHjdsAeio",
        "outputId": "13bc7c6b-a813-45ab-b108-48561ea41cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.4383\n",
            "Epoch 2/10, Loss: 0.3717\n",
            "Epoch 3/10, Loss: 0.3482\n",
            "Epoch 4/10, Loss: 0.3362\n",
            "Epoch 5/10, Loss: 0.3244\n",
            "Epoch 6/10, Loss: 0.3139\n",
            "Epoch 7/10, Loss: 0.3077\n",
            "Epoch 8/10, Loss: 0.2904\n",
            "Epoch 9/10, Loss: 0.2790\n",
            "Epoch 10/10, Loss: 0.2681\n",
            "Elapsed:  18.18229651451111\n"
          ]
        }
      ],
      "source": [
        "model_nat = SentimentNet(d_vocab, d_embed, embedding_matrix, seq_length)\n",
        "model_nat = model_nat.to(device)\n",
        "if not os.path.exists('/content/drive/MyDrive/CS521/weights_IMDB_nat.pt'):\n",
        "    start_time = time.time()\n",
        "    train_IMDB_nat(model_nat, num_epochs=10)\n",
        "    end_time = time.time()\n",
        "    print(\"Elapsed: \", end_time - start_time)\n",
        "    torch.save(model_nat.state_dict(), '/content/drive/MyDrive/CS521/weights_IMDB_nat.pt')\n",
        "else:\n",
        "    model_nat.load_state_dict(torch.load('/content/drive/MyDrive/CS521/weights_IMDB_nat.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "SO40VlvdwsmS"
      },
      "outputs": [],
      "source": [
        "def robust_training_IMDB(model, num_epochs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    kappa = 1\n",
        "    eps = 0\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch_inputs, batch_labels in train_loader:\n",
        "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            batch_size = batch_inputs.shape[0]\n",
        "\n",
        "            outputs = model(batch_inputs)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                batch_inputs_embed = model.embedding(batch_inputs)\n",
        "            ibp_lb, ibp_ub = IBP_bounds_IMDB(model, batch_inputs_embed, eps)\n",
        "            z = ibp_ub # this is a pointer, careful\n",
        "            z[torch.arange(batch_size), batch_labels] = ibp_lb[torch.arange(batch_size), batch_labels]\n",
        "            z = torch.softmax(z, dim=-1)\n",
        "            loss = kappa * criterion(outputs, batch_labels) + (1 - kappa) * criterion(z, batch_labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.3f}')\n",
        "\n",
        "        kappa -= 0.5 / (num_epochs-1)\n",
        "        eps += (emb_range * 0.05) / (num_epochs-1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(\"Elapsed: \", end_time - start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbB6S0b0LGHq",
        "outputId": "5d73791c-5db1-45cc-82c1-7c49ad0e3484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.444\n",
            "Epoch 2/10, Loss: 0.425\n",
            "Epoch 3/10, Loss: 0.455\n",
            "Epoch 4/10, Loss: 0.499\n",
            "Epoch 5/10, Loss: 0.543\n",
            "Epoch 6/10, Loss: 0.592\n",
            "Epoch 7/10, Loss: 0.640\n",
            "Epoch 8/10, Loss: 0.691\n",
            "Epoch 9/10, Loss: 0.742\n",
            "Epoch 10/10, Loss: 0.792\n",
            "Elapsed:  45.383729696273804\n"
          ]
        }
      ],
      "source": [
        "model_ibp = SentimentNet(d_vocab, d_embed, embedding_matrix, seq_length)\n",
        "model_ibp = model_ibp.to(device)\n",
        "robust_training_IMDB(model_ibp, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS5VSVy6DDfE",
        "outputId": "7b0ebe19-8625-472e-a096-9b1c10113bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naturally trained results\n",
            "Natural accuracy: 0.85876\n",
            "\n",
            "Robust accuracy on PGD 0.08334600448608398 images: 44.492\n",
            "Robust accuracy on PGD 0.16669200897216796 images: 8.576\n",
            "Robust accuracy on PGD 0.25003801345825194 images: 0.02\n",
            "Robust accuracy on PGD 0.3333840179443359 images: 0.0\n",
            "Robust accuracy on PGD 0.41673002243041996 images: 0.0\n",
            "\n",
            "Verified accuracy: 67.432\n",
            "Verified accuracy: 41.364\n",
            "Verified accuracy: 18.652\n",
            "Verified accuracy: 5.876\n",
            "Verified accuracy: 1.212\n"
          ]
        }
      ],
      "source": [
        "print(\"Naturally trained results\")\n",
        "test_accuracy = test_natural_acc(model_nat, test_loader)\n",
        "print()\n",
        "# epsilon should be around this order of magnitude, considering we use around epsilon=0.1 for MNIST of [0,1]\n",
        "emb_range = (embedding_matrix.max() - embedding_matrix.min()).item()\n",
        "for eps_test in [0.01,0.02,0.03,0.04,0.05]:\n",
        "    epsilon = eps_test * emb_range\n",
        "    test_IMDB_model_on_PGD(model_nat, eps=epsilon)\n",
        "print()\n",
        "for eps_test in [0.01,0.02,0.03,0.04,0.05]:\n",
        "    epsilon = eps_test * emb_range / 100\n",
        "    test_model_verified_accuracy(model_nat, eps=epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6jeCu_zT2v2",
        "outputId": "b73ee553-8f2b-4773-def1-537c409a6884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust trained results\n",
            "Natural accuracy: 0.85664\n",
            "\n",
            "Robust accuracy on PGD 0.08334600448608398 images: 44.324\n",
            "Robust accuracy on PGD 0.16669200897216796 images: 33.004\n",
            "Robust accuracy on PGD 0.25003801345825194 images: 16.676\n",
            "Robust accuracy on PGD 0.3333840179443359 images: 1.148\n",
            "Robust accuracy on PGD 0.41673002243041996 images: 0.004\n",
            "\n",
            "Verified accuracy: 68.46\n",
            "Verified accuracy: 43.368\n",
            "Verified accuracy: 20.652\n",
            "Verified accuracy: 6.952\n",
            "Verified accuracy: 1.496\n"
          ]
        }
      ],
      "source": [
        "print(\"Robust trained results\")\n",
        "test_natural_acc(model_ibp, test_loader)\n",
        "print()\n",
        "emb_range = (embedding_matrix.max() - embedding_matrix.min()).item()\n",
        "for eps_test in [0.01,0.02,0.03,0.04,0.05]:\n",
        "    epsilon = eps_test * emb_range\n",
        "    test_IMDB_model_on_PGD(model_ibp, eps=epsilon)\n",
        "print()\n",
        "for eps_test in [0.01,0.02,0.03,0.04,0.05]:\n",
        "    epsilon = eps_test * emb_range / 100\n",
        "    test_model_verified_accuracy(model_ibp, eps=epsilon)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Whsg1XX_OZs6",
        "ZPMdfEhtR3zm",
        "2XQ6tDCOX7LI",
        "4GKddK_67XjN",
        "9E2xFaPC13HK",
        "9__K7LnmnnDk"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}